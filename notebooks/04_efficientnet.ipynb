{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: EfficientNet Transfer Learning\n",
    "\n",
    "**Project:** AI-Powered Pneumonia Detection from Chest X-Rays  \n",
    "**Author:** Georgios Kitsakis  \n",
    "**Date:** 2025-10-30\n",
    "\n",
    "## Objectives\n",
    "1. Implement EfficientNet-B3 with transfer learning\n",
    "2. Compare performance with ResNet50 baseline\n",
    "3. Achieve higher accuracy with more efficient architecture\n",
    "4. Demonstrate knowledge of modern CNN architectures\n",
    "5. Save best performing model\n",
    "\n",
    "## Why EfficientNet?\n",
    "- **State-of-the-art**: Published by Google Research (2019)\n",
    "- **Efficient**: Better accuracy with fewer parameters than ResNet\n",
    "- **Scalable**: Compound scaling method balances depth, width, resolution\n",
    "- **Medical Imaging**: Proven excellent results on medical datasets\n",
    "- **Modern**: Shows knowledge of cutting-edge architectures\n",
    "\n",
    "## Expected Improvements\n",
    "- ResNet50 achieved: **89.58% test accuracy**\n",
    "- EfficientNet-B3 target: **91-93% test accuracy**\n",
    "- Faster inference time\n",
    "- Better gradient flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = '../data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0  # Set to 0 for Windows\n",
    "IMAGE_SIZE = 300  # EfficientNet-B3 uses 300x300\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading chest X-ray images\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpeg', '.jpg', '.png')):\n",
    "                        self.images.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Grayscale\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Transformations\n",
    "\n",
    "EfficientNet uses 300x300 images and ImageNet normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"✓ Transforms defined\")\n",
    "print(f\"  Train: Resize({IMAGE_SIZE}), Augmentation, Normalize\")\n",
    "print(f\"  Val/Test: Resize({IMAGE_SIZE}), Normalize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(TRAIN_DIR, transform=train_transform)\n",
    "val_dataset = ChestXrayDataset(VAL_DIR, transform=val_transform)\n",
    "test_dataset = ChestXrayDataset(TEST_DIR, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Dataset Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"Total samples: {len(train_dataset) + len(val_dataset) + len(test_dataset):,}\")\n",
    "print(f\"\\nBatches per epoch (train): {len(train_loader)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build EfficientNet-B3 Model\n",
    "\n",
    "EfficientNet-B3 architecture with transfer learning from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained EfficientNet-B3\n",
    "model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze early layers (fine-tune only final layers)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last few blocks for fine-tuning\n",
    "for param in model.features[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace classifier for binary classification\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EfficientNet-B3 Model Architecture\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Pre-trained weights: ImageNet\")\n",
    "print(f\"Input size: {IMAGE_SIZE}x{IMAGE_SIZE}x3\")\n",
    "print(f\"Output classes: 2 (NORMAL, PNEUMONIA)\")\n",
    "print(f\"\\nCustom Classifier:\")\n",
    "print(model.classifier)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Trainable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Function and Optimizer\n",
    "\n",
    "Using weighted cross-entropy to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced dataset\n",
    "train_labels = train_dataset.labels\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(f\"  NORMAL: {class_counts[0]:,} ({class_counts[0]/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  PNEUMONIA: {class_counts[1]:,} ({class_counts[1]/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"\\nClass weights: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer (Adam with weight decay)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Loss function and optimizer configured\")\n",
    "print(f\"  Loss: CrossEntropyLoss (weighted)\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (patience=2, factor=0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"✓ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_acc = 0.0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Training - EfficientNet-B3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: Beat ResNet50's 89.58% test accuracy\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - {epoch_time:.1f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, '../models/efficientnet_b3_pneumonia.pth')\n",
    "        print(f\"  ✓ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total training time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Model saved: ../models/efficientnet_b3_pneumonia.pth\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('EfficientNet-B3: Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax2.plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax2.axhline(y=89.58, color='g', linestyle='--', label='ResNet50 (89.58%)', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('EfficientNet-B3: Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/efficientnet_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved to reports/efficientnet_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('../models/efficientnet_b3_pneumonia.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model from checkpoint\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Val Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"  Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Test Set Results - EfficientNet-B3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"\\nComparison with ResNet50:\")\n",
    "print(f\"  ResNet50: 89.58%\")\n",
    "print(f\"  EfficientNet-B3: {test_acc:.2f}%\")\n",
    "print(f\"  Improvement: {test_acc - 89.58:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Detailed Metrics and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for confusion matrix\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            annot_kws={'size': 16})\n",
    "plt.title('EfficientNet-B3: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/efficientnet_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Classification Report - EfficientNet-B3\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)  # Recall for PNEUMONIA\n",
    "specificity = tn / (tn + fp)  # Recall for NORMAL\n",
    "\n",
    "print(\"\\nMedical Metrics:\")\n",
    "print(f\"  Sensitivity (Recall for PNEUMONIA): {sensitivity*100:.2f}%\")\n",
    "print(f\"  Specificity (Recall for NORMAL): {specificity*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['ResNet50', 'EfficientNet-B3'],\n",
    "    'Test Accuracy (%)': [89.58, test_acc],\n",
    "    'Parameters (M)': [25.6, 12.2],  # Approximate\n",
    "    'Image Size': ['224x224', '300x300'],\n",
    "    'Training Time (min)': [226, total_time/60]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Model Comparison: ResNet50 vs EfficientNet-B3\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Determine winner\n",
    "if test_acc > 89.58:\n",
    "    print(f\"\\n🎉 EfficientNet-B3 WINS! +{test_acc - 89.58:.2f}% improvement!\")\n",
    "else:\n",
    "    print(f\"\\nResNet50 still leads by {89.58 - test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Phase 5 Complete: EfficientNet-B3 Training and Evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
